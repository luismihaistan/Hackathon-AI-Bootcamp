import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import joblib
import os

# --- CONFIGURARE CÄ‚I (PATHS) ---
# AflÄƒm unde se aflÄƒ acest script (Ã®n folderul ai_model)
current_dir = os.path.dirname(os.path.abspath(__file__))
# Mergem un nivel mai sus (Ã®n root)
root_dir = os.path.dirname(current_dir)
# Construim calea cÄƒtre CSV È™i cÄƒtre folderul de modele
DATA_PATH = os.path.join(root_dir, 'data', 'creditcard_mini.csv')
MODELS_DIR = os.path.join(root_dir, 'models')

# 1. ÃNCÄ‚RCARE DATE DIN CSV

def incarca_date():
    print(f"ğŸ“‚ CÄƒutÄƒm fiÈ™ierul la: {DATA_PATH}")
    
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"âŒ Eroare: Nu am gÄƒsit fiÈ™ierul 'creditcard_mini.csv' la calea specificatÄƒ.")
    
    df = pd.read_csv(DATA_PATH)
    
    # --- FILTRU PENTRU MICRO-TRANZACÈšII ---
    # Definim limita (ex: 50.00 unitÄƒÈ›i monetare)
    LIMITA_MICRO = 100.0
    
    print(f"ğŸ“Š Total tranzacÈ›ii iniÈ›iale: {len(df)}")
    print(f"ğŸ’° Suma maximÄƒ existentÄƒ: {df['Amount'].max()}")
    
    # PÄƒstrÄƒm doar ce e sub limitÄƒ
    df = df[df['Amount'] <= LIMITA_MICRO]
    
    print(f"âœ‚ï¸  AplicÄƒm filtru micro-tranzacÈ›ii (<= {LIMITA_MICRO})")
    print(f"âœ… TranzacÈ›ii rÄƒmase: {len(df)}")
    
    # VerificÄƒm coloana target
    if 'is_fraud' not in df.columns:
        if 'Class' in df.columns:
            df.rename(columns={'Class': 'is_fraud'}, inplace=True)
        else:
            raise ValueError("âŒ CSV-ul nu conÈ›ine o coloanÄƒ 'is_fraud' sau 'Class'.")
            
    print(f"Procentaj fraude Ã®n datele filtrate: {df['is_fraud'].mean()*100:.2f}%")
    return df

# 2. PREPROCESARE
def preprocesare_date(df):
    # EliminÄƒm coloane care nu ajutÄƒ modelul (ID-uri, Nume, etc.)
    # errors='ignore' Ã®nseamnÄƒ cÄƒ nu dÄƒ eroare dacÄƒ coloana nu existÄƒ
    cols_to_drop = ['user_id', 'id', 'Time', 'Unnamed: 0']
    X = df.drop(['is_fraud'] + cols_to_drop, axis=1, errors='ignore')
    
    y = df['is_fraud']
    
    print(f"Features folosite pentru antrenare: {list(X.columns)}")
    
    # Scalarea datelor
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ReturnÄƒm scaler-ul ca sÄƒ Ã®l putem salva
    return X_scaled, y, scaler

# 3. ANTRENARE MODEL
def antreneaza_model():
    print("--- Ãncepere Proces Antrenare (Date Reale) ---")
    
    # A. ÃncÄƒrcare date
    try:
        df = incarca_date()
    except Exception as e:
        print(e)
        return # Opresc execuÈ›ia dacÄƒ nu pot Ã®ncÄƒrca datele
    
    # B. Preprocesare
    X, y, scaler = preprocesare_date(df)
    
    # C. ÃmpÄƒrÈ›ire Train / Test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # D. IniÈ›ializare Model
    print("ğŸš€ IniÈ›ializare Random Forest...")
    model = RandomForestClassifier(
        n_estimators=100, 
        max_depth=10, 
        random_state=42, 
        class_weight='balanced', # Important pentru date dezechilibrate (frauda e rarÄƒ)
        n_jobs=-1
    )
    
    # E. Fitting
    print("â³ Se antreneazÄƒ modelul...")
    model.fit(X_train, y_train)
    
    # F. Evaluare
    print("\n--- Rezultate Evaluare (Pe datele de test) ---")
    y_pred = model.predict(X_test)
    
    print("Matrice de confuzie:")
    print(confusion_matrix(y_test, y_pred))
    print("\nRaport Detaliat:")
    print(classification_report(y_test, y_pred))
    
    # G. Salvare Model È™i Scaler
    if not os.path.exists(MODELS_DIR):
        os.makedirs(MODELS_DIR)
        
    model_save_path = os.path.join(MODELS_DIR, 'fraud_model.pkl')
    scaler_save_path = os.path.join(MODELS_DIR, 'scaler.pkl')

    joblib.dump(model, model_save_path)
    joblib.dump(scaler, scaler_save_path)
    print(f"\nâœ… Succes! Modelul a fost salvat Ã®n: {MODELS_DIR}")

if __name__ == "__main__":
    antreneaza_model()